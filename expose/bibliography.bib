% This file was created with Citavi 6.10.0.0

@proceedings{.2013,
 year = {2013},
 title = {2013 IEEE International Conference on Robotics and Automation},
 publisher = {IEEE},
 isbn = {978-1-4673-5643-5}
}


@book{AbuMostafa.2012,
 author = {Abu-Mostafa, Yaser S. and Magdon-Ismail, Malik and Lin, Hsuan-Tien},
 year = {2012},
 title = {Learning from data: A short course},
 price = {hardcover: 28,00 USD},
 address = {S.l.},
 publisher = {AMLbook.com},
 isbn = {1600490069},
 file = {Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin - Learning From Data{\_} A short course-AMLBook.com (2012):Attachments/Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin - Learning From Data{\_} A short course-AMLBook.com (2012).pdf:application/pdf}
}


@article{Allier.2022,
 author = {Allier, C{\'e}dric and Herv{\'e}, Lionel and Paviolo, Chiara and Mandula, Ondrej and Cioni, Olivier and Pierr{\'e}, William and Andriani, Francesca and Padmanabhan, Kiran and Morales, Sophie},
 year = {2022},
 title = {CNN-Based Cell Analysis: From Image to Quantitative Representation},
 keywords = {cell analysis},
 volume = {9},
 journal = {Frontiers in Physics},
 doi = {10.3389/fphy.2021.776805},
 file = {Allier, Herv{\'e} et al. 2022 - CNN-Based Cell Analysis:Attachments/Allier, Herv{\'e} et al. 2022 - CNN-Based Cell Analysis.pdf:application/pdf}
}


@www{AnalyticsVidhya.2021,
 abstract = {Tutorial:~CNN is a powerful algorithm for image processing. This article is a beginners guide to image processing using Convolutional Neural Networks},
 author = {{Analytics Vidhya}},
 year = {2021},
 title = {Image Processing using CNN | Beginner's Guide to Image Processing},
 url = {https://www.analyticsvidhya.com/blog/2021/06/image-processing-using-cnn-a-beginners-guide/},
 urldate = {2022-03-18}
}


@inproceedings{Baskar.2018,
 abstract = {Automatic Facial Expression analysis has enthralled increasing attention in the research community in excess of two decades and its expedient in many application like, face animation, customer satisfaction studies, human-computer interaction and video conferencing. The precisely classifying different emotion is an essential problem in facial expression recognition research. There are several machine learning algorithms applied to facial expression recognition expedition. In this paper, we surveyed three different machine learning algorithms such as Bayesian Network, Hidden Markov Model and Support Vector machine and we attempt to answer following questions: How classification algorithm used its characteristics for emotion recognition? How various parameters in learning algorithm is devoted for better classification? What are the robust features used for training? Finally, we examined how advances in machine learning technique used for facial expression recognition?},
 author = {Baskar, A. and {Gireesh Kumar}, T.},
 title = {Facial Expression Classification Using Machine Learning Approach: A Review},
 keywords = {Facial expression;robot vision},
 pages = {337--345},
 bookpagination = {page},
 publisher = {{Springer Singapore}},
 isbn = {978-981-10-3223-3},
 editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Raju, K. Srujan and Janakiramaiah, B.},
 booktitle = {Data Engineering and Intelligent Computing},
 year = {2018},
 address = {Singapore}
}


@book{Bishop.2009,
 author = {Bishop, Christopher M.},
 year = {2009},
 title = {Pattern recognition and machine learning},
 address = {New York},
 edition = {9. (corrected at 8th printing)},
 publisher = {Springer},
 isbn = {9780387310732},
 series = {Information science and statistics}
}


@misc{Choi.2021,
 abstract = {Most deep-learning frameworks for understanding biological swarms are designed to fit perceptive models of group behavior to individual-level data (e.g., spatial coordinates of identified features of individuals) that have been separately gathered from video observations. Despite considerable advances in automated tracking, these methods are still very expensive or unreliable when tracking large numbers of animals simultaneously. Moreover, this approach assumes that the human-chosen features include sufficient features to explain important patterns in collective behavior. To address these issues, we propose training deep network models to predict system-level states directly from generic graphical features from the entire view, which can be relatively inexpensive to gather in a completely automated fashion. Because the resulting predictive models are not based on human-understood predictors, we use explanatory modules (e.g., Grad-CAM) that combine information hidden in the latent variables of the deep-network model with the video data itself to communicate to a human observer which aspects of observed individual behaviors are most informative in predicting group behavior. This represents an example of augmented intelligence in behavioral ecology -- knowledge co-creation in a human-AI team. As proof of concept, we utilize a 20-day video recording of a colony of over 50 Harpegnathos saltator ants to showcase that, without any individual annotations provided, a trained model can generate an {\textquotedbl}importance map{\textquotedbl} across the video frames to highlight regions of important behaviors, such as dueling (which the AI has no a priori knowledge of), that play a role in the resolution of reproductive-hierarchy re-formation. Based on the empirical results, we also discuss the potential use and current challenges.

Best Paper Winner at the 4th International Symposium on Swarm Behavior and Bio-Inspired Robotics (SWARM 2021)},
 author = {Choi, Taeyeong and Pyenson, Benjamin and Liebig, Juergen and Pavlic, Theodore P.},
 date = {2021},
 title = {Beyond Tracking: Using Deep Learning to Discover Novel Interactions in Biological Swarms},
 keywords = {animal observation;cell analysis},
 publisher = {arXiv},
 doi = {10.48550/arXiv.2108.09394}
}


@article{Di.2010,
 author = {Di, Wei and Zhang, Lei and Zhang, David and Pan, Quan},
 year = {2010},
 title = {Studies on Hyperspectral Face Recognition in Visible Spectrum With Feature Band Selection},
 pages = {1354--1361},
 pagination = {page},
 volume = {40},
 number = {6},
 issn = {1083-4427},
 journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
 doi = {10.1109/TSMCA.2010.2052603},
 file = {Di, Zhang et al. 2010 - Studies on Hyperspectral Face Recognition:Attachments/Di, Zhang et al. 2010 - Studies on Hyperspectral Face Recognition.pdf:application/pdf}
}


@inproceedings{Fanello.2013,
 abstract = {They make a robot track and recognize different objects by using independent motion detection. They predict robot egomotion by using a heteroscedastic learning algorhithm.



Object recognition framework is used to automatically label objects.},
 author = {Fanello, Sean Ryan and Ciliberto, Carlo and Natale, Lorenzo and Metta, Giorgio},
 title = {Weakly supervised strategies for natural object recognition in robotics},
 keywords = {heteroscedastic learning algorhithm;robot vision},
 pages = {4223--4229},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-4673-5643-5},
 booktitle = {2013 IEEE International Conference on Robotics and Automation},
 year = {2013},
 doi = {10.1109/ICRA.2013.6631174},
 file = {Weakly{\_}supervised{\_}strategies{\_}natural{\_}obj{\_}reco{\_}robotics:Attachments/Weakly{\_}supervised{\_}strategies{\_}natural{\_}obj{\_}reco{\_}robotics.pdf:application/pdf}
}


@article{FDA.20181118,
 abstract = {FDA permits marketing of first medical device to use artificial intelligence to detect greater than a mild level of diabetic retinopathy in the eye of adults who have diabetes.},
 year = {2018-11-18},
 title = {FDA permits marketing of artificial intelligence-based device to detect certain diabetes-related eye problems},
 url = {https://www.fda.gov/news-events/press-announcements/fda-permits-marketing-artificial-intelligence- based-device-detect- certain-diabetes-related-eye},
 urldate = {2022-03-22},
 journal = {U.S. Food and Drug Administration (FDA)},
 file = {FDA permits marketing of artificial time datetime= 2020-03-24T22 17 52Z Tue, 03 24 2020 - 22 17 time:Attachments/FDA permits marketing of artificial time datetime= 2020-03-24T22 17 52Z Tue, 03 24 2020 - 22 17 time.pdf:application/pdf}
}


@article{Godinez.2017,
 abstract = {MOTIVATION

Identifying phenotypes based on high-content cellular images is challenging. Conventional image analysis pipelines for phenotype identification comprise multiple independent steps, with each step requiring method customization and adjustment of multiple parameters.

RESULTS

Here, we present an approach based on a multi-scale convolutional neural network (M-CNN) that classifies, in a single cohesive step, cellular images into phenotypes by using directly and solely the images' pixel intensity values. The only parameters in the approach are the weights of the neural network, which are automatically optimized based on training images. The approach requires no a priori knowledge or manual customization, and is applicable to single- or multi-channel images displaying single or multiple cells. We evaluated the classification performance of the approach on eight diverse benchmark datasets. The approach yielded overall a higher classification accuracy compared with state-of-the-art results, including those of other deep CNN architectures. In addition to using the network to simply obtain a yes-or-no prediction for a given phenotype, we use the probability outputs calculated by the network to quantitatively describe the phenotypes. This study shows that these probability values correlate with chemical treatment concentrations. This finding validates further our approach and enables chemical treatment potency estimation via CNNs.

AVAILABILITY AND IMPLEMENTATION

The network specifications and solver definitions are provided in Supplementary Software 1.

CONTACT

william{\_}jose.godinez{\_}navarro@novartis.com or xian-1.zhang@novartis.com.

SUPPLEMENTARY INFORMATION

Supplementary data are available at Bioinformatics online.},
 author = {Godinez, William J. and Hossain, Imtiaz and Lazic, Stanley E. and Davies, John W. and Zhang, Xian},
 year = {2017},
 title = {A multi-scale convolutional neural network for phenotyping high-content cellular images},
 keywords = {cell analysis},
 pages = {2010--2019},
 pagination = {page},
 volume = {33},
 number = {13},
 journal = {Bioinformatics (Oxford, England)},
 doi = {10.1093/bioinformatics/btx069},
 file = {Godinez, Hossain et al. 2017 - A multi-scale convolutional neural network:Attachments/Godinez, Hossain et al. 2017 - A multi-scale convolutional neural network.pdf:application/pdf}
}


@article{Graving.2019,
 abstract = {Quantitative behavioral measurements are important for answering questions across scientific disciplines-from neuroscience to ecology. State-of-the-art deep-learning methods offer major advances in data quality and detail by allowing researchers to automatically estimate locations of an animal's body parts directly from images or videos. However, currently available animal pose estimation methods have limitations in speed and robustness. Here, we introduce a new easy-to-use software toolkit, DeepPoseKit, that addresses these problems using an efficient multi-scale deep-learning model, called Stacked DenseNet, and a fast GPU-based peak-detection algorithm for estimating keypoint locations with subpixel precision. These advances improve processing speed {\textgreater}2x with no loss in accuracy compared to currently available methods. We demonstrate the versatility of our methods with multiple challenging animal pose estimation tasks in laboratory and field settings-including groups of interacting individuals. Our work reduces barriers to using advanced tools for measuring behavior and has broad applicability across the behavioral sciences.},
 author = {Graving, Jacob M. and Chae, Daniel and Naik, Hemal and Li, Liang and Koger, Benjamin and Costelloe, Blair R. and Couzin, Iain D.},
 year = {2019},
 title = {DeepPoseKit, a software toolkit for fast and robust animal pose estimation using deep learning},
 keywords = {animal observation;cell analysis;nature},
 volume = {8},
 journal = {eLife},
 doi = {10.7554/eLife.47994}
}


@book{Hastie.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
 year = {2009},
 title = {The elements of statistical learning: Data mining, inference, and prediction},
 price = {EUR 74.85},
 address = {New York},
 edition = {Second edition},
 publisher = {Springer},
 isbn = {978-0-387-84857-0},
 series = {Springer series in statistics},
 file = {Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni:Attachments/Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni.pdf:application/pdf}
}


@article{Ho.2019,
 author = {Ho, Thi Kieu and Gwak, Jeonghwan},
 year = {2019},
 title = {Multiple Feature Integration for Classification of Thoracic Disease in Chest Radiography},
 keywords = {covid-19},
 pages = {4130},
 pagination = {page},
 volume = {9},
 number = {19},
 journal = {Applied Sciences},
 doi = {10.3390/app9194130},
 file = {Ho, Gwak 2019 - Multiple Feature Integration for Classification:Attachments/Ho, Gwak 2019 - Multiple Feature Integration for Classification.pdf:application/pdf}
}


@inproceedings{Hotwani.2018,
 author = {Hotwani, Kamal and Agarwal, Sanjeev and Paswan, Roshan},
 title = {Hybrid Models for Offline Handwritten Character Recognition System Without Using any Prior Database Images},
 pages = {99--108},
 bookpagination = {page},
 publisher = {{Springer Singapore}},
 isbn = {978-981-10-3223-3},
 editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Raju, K. Srujan and Janakiramaiah, B.},
 booktitle = {Data Engineering and Intelligent Computing},
 year = {2018},
 address = {Singapore}
}


@proceedings{IEEERoboticsandAutomationSociety.2002,
 year = {2002},
 title = {2002 IEEE International Conference on Robotics and Automation: May 11 - 15, 2002, Washington, D.C. ; proceedings},
 keywords = {human robot interaction},
 address = {Piscataway, NJ},
 publisher = {{IEEE Service Center}},
 isbn = {0-7803-7272-7},
 institution = {{IEEE Robotics and Automation Society} and {Institute of Electrical and Electronics Engineers}}
}


@proceedings{InstituteofElectricalandElectronicsEngineers.2016,
 year = {2016},
 title = {19th International Conference on Computer and Information Technology: 18-20 December 2016 : North South University},
 address = {Piscataway, NJ},
 publisher = {IEEE},
 isbn = {978-1-5090-4090-2},
 institution = {{Institute of Electrical and Electronics Engineers} and {North South University}}
}


@article{Ismael.2021,
 abstract = {COVID-19 is a novel virus that causes infection in both the upper respiratory tract and the lungs. The numbers of cases and deaths have increased on a daily basis on the scale of a global pandemic. Chest X-ray images have proven useful for monitoring various lung diseases and have recently been used to monitor the COVID-19 disease. In this paper, deep-learning-based approaches, namely deep feature extraction, fine-tuning of pretrained convolutional neural networks (CNN), and end-to-end training of a developed CNN model, have been used in order to classify COVID-19 and normal (healthy) chest X-ray images. For deep feature extraction, pretrained deep CNN models (ResNet18, ResNet50, ResNet101, VGG16, and VGG19) were used. For classification of the deep features, the Support Vector Machines (SVM) classifier was used with various kernel functions, namely Linear, Quadratic, Cubic, and Gaussian. The aforementioned pretrained deep CNN models were also used for the fine-tuning procedure. A new CNN model is proposed in this study with end-to-end training. A dataset containing 180 COVID-19 and 200 normal (healthy) chest X-ray images was used in the study's experimentation. Classification accuracy was used as the performance measurement of the study. The experimental works reveal that deep learning shows potential in the detection of COVID-19 based on chest X-ray images. The deep features extracted from the ResNet50 model and SVM classifier with the Linear kernel function produced a 94.7{\%} accuracy score, which was the highest among all the obtained results. The achievement of the fine-tuned ResNet50 model was found to be 92.6{\%}, whilst end-to-end training of the developed CNN model produced a 91.6{\%} result. Various local texture descriptors and SVM classifications were also used for performance comparison with alternative deep approaches; the results of which showed the deep approaches to be quite efficient when compared to the local texture descriptors in the detection of COVID-19 based on chest X-ray images.},
 author = {Ismael, Aras M. and {\c{S}}eng{\"u}r, Abdulkadir},
 year = {2021},
 title = {Deep learning approaches for COVID-19 detection based on chest X-ray images},
 keywords = {covid-19},
 pages = {114054},
 pagination = {page},
 volume = {164},
 issn = {0957-4174},
 journal = {Expert systems with applications},
 doi = {10.1016/j.eswa.2020.114054},
 file = {2021{\_}Deep learning approaches for COVID-19 detection based on chest:Attachments/2021{\_}Deep learning approaches for COVID-19 detection based on chest.pdf:application/pdf}
}


@article{Karakanis.2021,
 abstract = {Deep learning methods have already enjoyed an unprecedented success in medical imaging problems. Similar success has been evidenced when it comes to the detection of COVID-19 from medical images, therefore deep learning approaches are considered good candidates for detecting this disease, in collaboration with radiologists and/or physicians. In this paper, we propose a new approach to detect COVID-19 via exploiting a conditional generative adversarial network to generate synthetic images for augmenting the limited amount of data available. Additionally, we propose two deep learning models following a lightweight architecture, commensurating with the overall amount of data available. Our experiments focused on both binary classification for COVID-19 vs Normal cases and multi-classification that includes a third class for bacterial pneumonia. Our models achieved a competitive performance compared to other studies in literature and also a ResNet8 model. Our best performing binary model achieved 98.7{\%} accuracy, 100{\%} sensitivity and 98.3{\%} specificity, while our three-class model achieved 98.3{\%} accuracy, 99.3{\%} sensitivity and 98.1{\%} specificity. Moreover, via adopting a testing protocol proposed in literature, our models proved to be more robust and reliable in COVID-19 detection than a baseline ResNet8, making them good candidates for detecting COVID-19 from posteroanterior chest X-ray images.},
 author = {Karakanis, Stefanos and Leontidis, Georgios},
 year = {2021},
 title = {Lightweight deep learning models for detecting COVID-19 from chest X-ray images},
 keywords = {covid-19},
 pages = {104181},
 pagination = {page},
 volume = {130},
 journal = {Computers in biology and medicine},
 doi = {10.1016/j.compbiomed.2020.104181},
 file = {Karakanis, Leontidis 2021 - Lightweight deep learning models:Attachments/Karakanis, Leontidis 2021 - Lightweight deep learning models.pdf:application/pdf}
}


@article{Khan.2018,
 author = {Khan, Muhammad Jaleed and Khan, Hamid Saeed and Yousaf, Adeel and Khurshid, Khurram and Abbas, Asad},
 year = {2018},
 title = {Modern Trends in Hyperspectral Image Analysis: A Review},
 pages = {14118--14129},
 pagination = {page},
 volume = {6},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2018.2812999},
 file = {Modern{\_}Trends{\_}in{\_}Hyperspectral{\_}Image{\_}Analysis{\_}A{\_}Review:Attachments/Modern{\_}Trends{\_}in{\_}Hyperspectral{\_}Image{\_}Analysis{\_}A{\_}Review.pdf:application/pdf}
}


@inproceedings{Kim.2006,
 author = {Kim, Kye Kyung and Kwak, Keun Chang and Ch, Su Young},
 title = {Gesture analysis for human-robot interaction},
 keywords = {gesture analysis;human robot interaction},
 pages = {4 pp-1827},
 bookpagination = {page},
 publisher = {{IEEE Tech. Activities}},
 isbn = {89-5519-129-4},
 booktitle = {Toward the era of ubiquitous networks and societies},
 year = {2006},
 address = {Piscataway, NJ},
 doi = {10.1109/ICACT.2006.206345}
}


@article{LeCun.1989,
 author = {{Le Cun}, Y. and Jackel, L. D. and Boser, B. and Denker, J. S. and Graf, H. P. and Guyon, I. and Henderson, D. and Howard, R. E. and Hubbard, W.},
 year = {1989},
 title = {Handwritten digit recognition: applications of neural network chips and automatic learning},
 keywords = {Skeletonization},
 pages = {41--46},
 pagination = {page},
 volume = {27},
 number = {11},
 issn = {0163-6804},
 journal = {IEEE Communications Magazine},
 doi = {10.1109/35.41400},
 file = {Handwritten{\_}digit{\_}recognition{\_}applications{\_}of{\_}neural{\_}network{\_}chips{\_}and{\_}automatic{\_}learning:Attachments/Handwritten{\_}digit{\_}recognition{\_}applications{\_}of{\_}neural{\_}network{\_}chips{\_}and{\_}automatic{\_}learning.pdf:application/pdf}
}


@article{Li.2020,
 author = {Li, Shan and Deng, Weihong},
 year = {2020},
 title = {Deep Facial Expression Recognition: A Survey},
 keywords = {Facial expression;robot vision},
 pages = {1},
 pagination = {page},
 journal = {IEEE Transactions on Affective Computing},
 doi = {10.1109/TAFFC.2020.2981446},
 file = {Li, Deng 2020 - Deep Facial Expression Recognition:Attachments/Li, Deng 2020 - Deep Facial Expression Recognition.pdf:application/pdf}
}


@article{Lu.2019,
 abstract = {Cellular microscopy images contain rich insights about biology. To extract this information, researchers use features, or measurements of the patterns of interest in the images. Here, we introduce a convolutional neural network (CNN) to automatically design features for fluorescence microscopy. We use a self-supervised method to learn feature representations of single cells in microscopy images without labelled training data. We train CNNs on a simple task that leverages the inherent structure of microscopy images and controls for variation in cell morphology and imaging: given one cell from an image, the CNN is asked to predict the fluorescence pattern in a second different cell from the same image. We show that our method learns high-quality features that describe protein expression patterns in single cells both yeast and human microscopy datasets. Moreover, we demonstrate that our features are useful for exploratory biological analysis, by capturing high-resolution cellular components in a proteome-wide cluster analysis of human proteins, and by quantifying multi-localized proteins and single-cell variability. We believe paired cell inpainting is a generalizable method to obtain feature representations of single cells in multichannel microscopy images.},
 author = {Lu, Alex X. and Kraus, Oren Z. and Cooper, Sam and Moses, Alan M.},
 year = {2019},
 title = {Learning unsupervised feature representations for single cell microscopy images with paired cell inpainting},
 keywords = {cell analysis},
 pages = {e1007348},
 pagination = {page},
 volume = {15},
 number = {9},
 journal = {PLoS computational biology},
 doi = {10.1371/journal.pcbi.1007348}
}


@article{Mathis.2018,
 abstract = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled ({\~{}}200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
 author = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M. and Abe, Taiga and Murthy, Venkatesh N. and Mathis, Mackenzie Weygandt and Bethge, Matthias},
 year = {2018},
 title = {DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},
 keywords = {animal observation;cell analysis;nature},
 pages = {1281--1289},
 pagination = {page},
 volume = {21},
 number = {9},
 journal = {Nature neuroscience},
 doi = {10.1038/s41593-018-0209-y}
}


@article{Moen.2019,
 abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
 author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and {van Valen}, David},
 year = {2019},
 title = {Deep learning for cellular image analysis},
 keywords = {cell analysis},
 pages = {1233--1246},
 pagination = {page},
 volume = {16},
 number = {12},
 journal = {Nature methods},
 doi = {10.1038/s41592-019-0403-1}
}


@article{Mohammed.2013,
 abstract = {Develop an image-guided adaptive robotic system for industrial applications.},
 author = {Mohammed, A. and Wang, L. and Gao, R. X.},
 year = {2013},
 title = {Integrated Image Processing and Path Planning for Robotic Sketching},
 pages = {199--204},
 pagination = {page},
 volume = {12},
 issn = {22128271},
 journal = {Procedia CIRP},
 doi = {10.1016/j.procir.2013.09.035}
}


@article{Narin.2021,
 abstract = {The 2019 novel coronavirus disease (COVID-19), with a starting point in China, has spread rapidly among people living in other countries, and is approaching approximately 34,986,502 cases worldwide according to the statistics of European Centre for Disease Prevention and Control. There are a limited number of COVID-19 test kits available in hospitals due to the increasing cases daily. Therefore, it is necessary to implement an automatic detection system as a quick alternative diagnosis option to prevent COVID-19 spreading among people. In this study, five pre-trained convolutional neural network based models (ResNet50, ResNet101, ResNet152, InceptionV3 and Inception-ResNetV2) have been proposed for the detection of coronavirus pneumonia infected patient using chest X-ray radiographs. We have implemented three different binary classifications with four classes (COVID-19, normal (healthy), viral pneumonia and bacterial pneumonia) by using 5-fold cross validation. Considering the performance results obtained, it has seen that the pre-trained ResNet50 model provides the highest classification performance (96.1{\%} accuracy for Dataset-1, 99.5{\%} accuracy for Dataset-2 and 99.7{\%} accuracy for Dataset-3) among other four used models.},
 author = {Narin, Ali and Kaya, Ceren and Pamuk, Ziynet},
 year = {2021},
 title = {Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images  and Deep Convolutional Neural Networks},
 url = {http://arxiv.org/pdf/2003.10849v3},
 keywords = {covid-19},
 pages = {1207--1220},
 pagination = {page},
 volume = {24},
 number = {3},
 issn = {1433-7541},
 journal = {Pattern Analysis and Applications},
 doi = {10.1007/s10044-021-00984-y}
}


@article{Ounkomol.2018,
 abstract = {Understanding cells as integrated systems is central to modern biology. Although fluorescence microscopy can resolve subcellular structure in living cells, it is expensive, is slow, and can damage cells. We present a label-free method for predicting three-dimensional fluorescence directly from transmitted-light images and demonstrate that it can be used to generate multi-structure, integrated images. The method can also predict immunofluorescence (IF) from electron micrograph (EM) inputs, extending the potential applications.},
 author = {Ounkomol, Chawin and Seshamani, Sharmishtaa and Maleckar, Mary M. and Collman, Forrest and Johnson, Gregory R.},
 year = {2018},
 title = {Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy},
 keywords = {cell analysis},
 pages = {917--920},
 pagination = {page},
 volume = {15},
 number = {11},
 journal = {Nature methods},
 doi = {10.1038/s41592-018-0111-2},
 file = {Ounkomol, Seshamani et al. 2018 - Label-free prediction of three-dimensional fluorescence:Attachments/Ounkomol, Seshamani et al. 2018 - Label-free prediction of three-dimensional fluorescence.pdf:application/pdf}
}


@article{Ozturk.2020,
 abstract = {The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08{\%} for binary classes and 87.02{\%} for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.},
 author = {Ozturk, Tulin and Talo, Muhammed and Yildirim, Eylul Azra and Baloglu, Ulas Baran and Yildirim, Ozal and {Rajendra Acharya}, U.},
 year = {2020},
 title = {Automated detection of COVID-19 cases using deep neural networks with X-ray images},
 keywords = {covid-19},
 pages = {103792},
 pagination = {page},
 volume = {121},
 journal = {Computers in biology and medicine},
 doi = {10.1016/j.compbiomed.2020.103792},
 file = {Ozturk, Talo et al. 2020 - Automated detection of COVID-19 cases:Attachments/Ozturk, Talo et al. 2020 - Automated detection of COVID-19 cases.pdf:application/pdf}
}


@article{Polsinelli.2020,
 abstract = {Computer Tomography (CT) imaging of the chest is a valid diagnosis tool to detect COVID-19 promptly and to control the spread of the disease. In this work we propose a light Convolutional Neural Network (CNN) design, based on the model of the SqueezeNet, for the efficient discrimination of COVID-19 CT images with respect to other community-acquired pneumonia and/or healthy CT images. The architecture allows to an accuracy of 85.03{\%} with an improvement of about 3.2{\%} in the first dataset arrangement and of about 2.1{\%} in the second dataset arrangement. The obtained gain, though of low entity, can be really important in medical diagnosis and, in particular, for Covid-19 scenario. Also the average classification time on a high-end workstation, 1.25~s, is very competitive with respect to that of more complex CNN designs, 13.41~s, witch require pre-processing. The proposed CNN can be executed on medium-end laptop without GPU acceleration in 7.81~s: this is impossible for methods requiring GPU acceleration. The performance of the method can be further improved with efficient pre-processing strategies for witch GPU acceleration is not necessary.},
 author = {Polsinelli, Matteo and Cinque, Luigi and Placidi, Giuseppe},
 year = {2020},
 title = {A light CNN for detecting COVID-19 from CT scans of the chest},
 keywords = {covid-19},
 pages = {95--100},
 pagination = {page},
 volume = {140},
 issn = {0167-8655},
 journal = {Pattern recognition letters},
 doi = {10.1016/j.patrec.2020.10.001},
 file = {Polsinelli, Cinque et al. 2020 - A light CNN for detecting:Attachments/Polsinelli, Cinque et al. 2020 - A light CNN for detecting.pdf:application/pdf}
}


@article{Pouly.2020,
 abstract = {BACKGROUND

Since 2017, there have been several reports of artificial intelligence (AI) achieving comparable performance to human experts on medical image analysis tasks. With the first ratification of a~computer vision algorithm as a~medical device in 2018, the way was paved for these methods to eventually become an integral part of modern clinical practice.

OBJECTIVES

The purpose of this article is to review the main developments that have occurred over the last few years in AI for image analysis, in relation to clinical applications and dermatology.

MATERIALS AND METHODS

Following the annual ImageNet challenge, we review classical methods of machine learning for image analysis and demonstrate how these methods incorporated human expertise but failed to meet industrial requirements regarding performance and scalability. With the rise of deep learning based on artificial neural networks, these limitations could be overcome. We discuss important aspects of this technology including transfer learning and report on recent developments such as explainable AI and generative models.

RESULTS

Deep learning models achieved performance on a~par with human experts in a~broad variety of diagnostic tasks and were shown to be suitable for industrialization. Therefore, current developments focus less on further improving accuracy but rather address open issues such as interpretability and applicability under clinical conditions. Upcoming generative models allow for entirely new applications.

CONCLUSIONS

Deep learning has a~history of remarkable success and has become the new technical standard for image analysis. The dramatic improvement these models brought over classical approaches enables applications in a~rapidly increasing number of clinical fields. In dermatology, as in many other domains, artificial intelligence still faces considerable challenges but is undoubtedly developing into an essential tool of modern medicine.},
 author = {Pouly, Marc and Koller, Thomas and Gottfrois, Philippe and Lionetti, Simone},
 year = {2020},
 title = {K{\"u}nstliche Intelligenz in der Bildanalyse -- Grundlagen und neue Entwicklungen},
 pages = {660--668},
 pagination = {page},
 volume = {71},
 number = {9},
 journal = {Der Hautarzt; Zeitschrift fur Dermatologie, Venerologie, und verwandte Gebiete},
 doi = {10.1007/s00105-020-04663-7},
 file = {Pouly, Koller et al. 2020 - K{\"u}nstliche Intelligenz in der Bildanalyse:Attachments/Pouly, Koller et al. 2020 - K{\"u}nstliche Intelligenz in der Bildanalyse.pdf:application/pdf}
}


@inproceedings{Prasad.2018,
 author = {Prasad, Priyanka and Gupta, Ashutosh},
 title = {Moving Object Tracking and Detection Based on Kalman Filter and Saliency Mapping},
 keywords = {object detection;robot vision},
 pages = {639--646},
 bookpagination = {page},
 publisher = {{Springer Singapore}},
 isbn = {978-981-10-3223-3},
 editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Raju, K. Srujan and Janakiramaiah, B.},
 booktitle = {Data Engineering and Intelligent Computing},
 year = {2018},
 address = {Singapore}
}


@inproceedings{Rao.2002,
 author = {Rao, R. S. and Conn, K. and Jung, S. H. and Katupitiya, J. and Kientz, T. and Kumar, V. and Ostrowski, J. and Patel, S. and Taylor, C. J.},
 title = {Human robot interaction: application to smart wheelchairs},
 keywords = {human robot interaction;robot vision},
 pages = {3583--3588},
 bookpagination = {page},
 publisher = {{IEEE Service Center}},
 isbn = {0-7803-7272-7},
 booktitle = {2002 IEEE International Conference on Robotics and Automation},
 year = {2002},
 address = {Piscataway, NJ},
 doi = {10.1109/ROBOT.2002.1014265},
 file = {Rao, Conn et al. 11-15 May 2002 - Human robot interaction:Attachments/Rao, Conn et al. 11-15 May 2002 - Human robot interaction.pdf:application/pdf}
}


@proceedings{Satapathy.2018,
 year = {2018},
 title = {Data Engineering and Intelligent Computing},
 keywords = {Facial expression;robot vision},
 address = {Singapore},
 publisher = {{Springer Singapore}},
 isbn = {978-981-10-3223-3},
 editor = {Satapathy, Suresh Chandra and Bhateja, Vikrant and Raju, K. Srujan and Janakiramaiah, B.},
 file = {2018{\_}Book{\_}DataEngineeringAndIntelligentC:Attachments/2018{\_}Book{\_}DataEngineeringAndIntelligentC.pdf:application/pdf}
}


@inproceedings{Schlenzig.1994,
 author = {Schlenzig, J. and Hunter, E. and Jain, R.},
 title = {Vision based hand gesture interpretation using recursive estimation},
 keywords = {gesture analysis;robot vision},
 pages = {1267--1271},
 bookpagination = {page},
 publisher = {{IEEE Computer Society Press}},
 isbn = {0-8186-6405-3},
 editor = {Singh, Avtar},
 booktitle = {Conference record of the Twenty-Eighth Asilomar Conference on Signals, Systems {\&} Computers},
 year = {1994},
 address = {Los Alamitos, Calif.},
 doi = {10.1109/ACSSC.1994.471662}
}


@inproceedings{Simul.2016,
 abstract = {They make a robot understand human gestures, facial expressions and gender.



Local binary pattern (LBP) and SVMs are used to classify human gender. SVMs in connection with landmark detection are used to detect facial expression.},
 author = {Simul, Nishikanto Sarkar and Ara, Nusrat Mubin and Islam, Md. Saiful},
 title = {A support vector machine approach for real time vision based human robot interaction},
 keywords = {robot vision;Support-Vector-Machines},
 pages = {496--500},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5090-4090-2},
 booktitle = {19th International Conference on Computer and Information Technology},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/ICCITECHN.2016.7860248}
}


@proceedings{Singh.1994,
 year = {1994},
 title = {Conference record of the Twenty-Eighth Asilomar Conference on Signals, Systems {\&} Computers: October 30 - November 2, 1994, Pacific Grove, California},
 address = {Los Alamitos, Calif.},
 publisher = {{IEEE Computer Society Press}},
 isbn = {0-8186-6405-3},
 editor = {Singh, Avtar},
 institution = {{Naval Postgraduate School} and {IEEE Signal Processing Society}}
}


@article{Souza.2019,
 abstract = {BACKGROUND AND OBJECTIVE

Chest X-ray (CXR) is one of the most used imaging techniques for detection and diagnosis of pulmonary diseases. A critical component in any computer-aided system, for either detection or diagnosis in digital CXR, is the automatic segmentation of the lung field. One of the main challenges inherent to this task is to include in the segmentation the lung regions overlapped by dense abnormalities, also known as opacities, which can be caused by diseases such as tuberculosis and pneumonia. This specific task is difficult because opacities frequently reach high intensity values which can be incorrectly interpreted by an automatic method as the lung boundary, and as a consequence, this creates a challenge in the segmentation process, because the chances of incomplete segmentations are increased considerably. The purpose of this work is to propose a method for automatic segmentation of lungs in CXR that addresses this problem by reconstructing the lung regions {\textquotedbl}lost{\textquotedbl} due to pulmonary abnormalities.

METHODS

The proposed method, which features two deep convolutional neural network models, consists of four steps main steps: (1) image acquisition, (2) initial segmentation, (3) reconstruction and (4) final segmentation.

RESULTS

The proposed method was experimented on 138 Chest X-ray images from Montgomery County's Tuberculosis Control Program, and has achieved as best result an average sensitivity of 97.54{\%}, an average specificity of 96.79{\%}, an average accuracy of 96.97{\%}, an average Dice coefficient of 94{\%}, and an average Jaccard index of 88.07{\%}.

CONCLUSIONS

We demonstrate in our lung segmentation method that the problem of dense abnormalities in Chest X-rays can be efficiently addressed by performing a reconstruction step based on a deep convolutional neural network model.},
 author = {Souza, Johnatan Carvalho and {Bandeira Diniz}, Jo{\~a}o Ot{\'a}vio and Ferreira, Jonnison Lima and {Da Fran{\c{c}}a Silva}, Giovanni Lucca and {Corr{\^e}a Silva}, Arist{\'o}fanes and de Paiva, Anselmo Cardoso},
 year = {2019},
 title = {An automatic method for lung segmentation and reconstruction in chest X-ray using deep neural networks},
 keywords = {covid-19},
 pages = {285--296},
 pagination = {page},
 volume = {177},
 journal = {Computer methods and programs in biomedicine},
 doi = {10.1016/j.cmpb.2019.06.005}
}


@proceedings{Sudkorea.2006,
 year = {2006},
 title = {Toward the era of ubiquitous networks and societies: The 8th International Conference on Advanced Communication Technology ; ICACT 2006 ; Phoenix Park, Korea, Feb. 20 - 22, 2006 ; proceedings},
 address = {Piscataway, NJ},
 publisher = {{IEEE Tech. Activities}},
 isbn = {89-5519-129-4},
 institution = {S{\"u}dkorea and Han'guk-Chŏnsanhoe and {IEEE Communications Society}}
}


@book{Thrun.2006,
 abstract = {An introduction to the techniques and algorithms of the newest field in robotics.},
 author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
 year = {2006},
 title = {Probabilistic robotics},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=6246533},
 address = {Cambridge, Massachusetts and London},
 publisher = {{MIT Press}},
 isbn = {9780262303804},
 series = {Intelligent robotics and autonomous agents}
}


@book{Tsai.2019,
 author = {Tsai, Hsieh-Fu and Gajda, Joanna and Sloan, Tyler F.W. and Rares, Andrei and Shen, Amy Q.},
 year = {2019},
 title = {Usiigaci: Instance-aware cell tracking in stain-free phase contrast microscopy enabled by machine learning},
 keywords = {cell analysis},
 doi = {10.1101/524041},
 file = {Tsai, Gajda et al. 2019 - Usiigaci Instance-aware cell tracking:Attachments/Tsai, Gajda et al. 2019 - Usiigaci Instance-aware cell tracking.pdf:application/pdf}
}


@www{Wikipedia.2022,
 abstract = {Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model. It is closely related to oversampling in data analysis.},
 editor = {Wikipedia},
 year = {2022},
 title = {Data augmentation},
 url = {https://en.wikipedia.org/w/index.php?title=Data_augmentation&oldid=1071104120},
 urldate = {2022-03-18},
 file = {Wikipedia (Hg.) 2022 - Data augmentation:Attachments/Wikipedia (Hg.) 2022 - Data augmentation.pdf:application/pdf}
}


@article{Wilson.1999,
 author = {Wilson, A. D. and Bobick, A. F.},
 year = {1999},
 title = {Parametric hidden Markov models for gesture recognition},
 keywords = {gesture analysis;robot vision},
 pages = {884--900},
 pagination = {page},
 volume = {21},
 number = {9},
 issn = {01628828},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 doi = {10.1109/34.790429}
}


@article{Wozniak.2018,
 abstract = {BACKGROUND AND OBJECTIVE

In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist's difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis.

METHODS

In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier.

RESULTS

The performance of our approach is 92{\%} of correct classifications, while the sensitivity is 95{\%} and the specificity is 89.7{\%}. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6{\%}) and true nodules with the false ones (2{\%}).

CONCLUSIONS

Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances.},
 author = {Wo{\'z}niak, Marcin and Po{\l}ap, Dawid and Capizzi, Giacomo and {Lo Sciuto}, Grazia and Ko{\'s}mider, Leon and Frankiewicz, Katarzyna},
 year = {2018},
 title = {Small lung nodules detection based on local variance analysis and probabilistic neural network},
 keywords = {covid-19},
 pages = {173--180},
 pagination = {page},
 volume = {161},
 journal = {Computer methods and programs in biomedicine},
 doi = {10.1016/j.cmpb.2018.04.025}
}


@article{Yan.2014,
 author = {Yan, Haibin and Ang, Marcelo H. and Poo, Aun Neow},
 year = {2014},
 title = {A Survey on Perception Methods for Human--Robot Interaction in Social Robots},
 keywords = {robot vision},
 pages = {85--119},
 pagination = {page},
 volume = {6},
 number = {1},
 issn = {1875-4791},
 journal = {International Journal of Social Robotics},
 doi = {10.1007/s12369-013-0199-6},
 file = {Yan2014{\_}Article{\_}ASurveyOnPerceptionMethodsForH:Attachments/Yan2014{\_}Article{\_}ASurveyOnPerceptionMethodsForH.pdf:application/pdf}
}


@article{Yang.2021,
 abstract = {The main purpose of this work is to investigate and compare several deep learning enhanced techniques applied to X-ray and CT-scan medical images for the detection of COVID-19. In this paper, we used four powerful pre-trained CNN models, VGG16, DenseNet121, ResNet50,and ResNet152, for the COVID-19 CT-scan binary classification task. The proposed Fast.AI ResNet framework was designed to find out the best architecture, pre-processing, and training parameters for the models largely automatically. The accuracy and F1-score were both above 96{\%} in the diagnosis of COVID-19 using CT-scan images. In addition, we applied transfer learning techniques to overcome the insufficient data and to improve the training time. The binary and multi-class classification of X-ray images tasks were performed by utilizing enhanced VGG16 deep transfer learning architecture. High accuracy of 99{\%} was achieved by enhanced VGG16 in the detection of X-ray images from COVID-19 and pneumonia. The accuracy and validity of the algorithms were assessed on X-ray and CT-scan well-known public datasets. The proposed methods have better results for COVID-19 diagnosis than other related in literature. In our opinion, our work can help virologists and radiologists to make a better and faster diagnosis in the struggle against the outbreak of COVID-19.},
 author = {Yang, Dandi and Martinez, Cristhian and Visu{\~n}a, Lara and Khandhar, Hardev and Bhatt, Chintan and Carretero, Jesus},
 year = {2021},
 title = {Detection and analysis of COVID-19 in medical images using deep learning techniques},
 keywords = {covid-19},
 pages = {19638},
 pagination = {page},
 volume = {11},
 number = {1},
 journal = {Scientific reports},
 doi = {10.1038/s41598-021-99015-3}
}


