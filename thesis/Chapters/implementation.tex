\chapter{Implementation} % Main chapter title

\label{chap:implementation} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref*{chap:implementation}. \emph{Implementation}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

% \begin{lstlisting}[label={lst:swapoff}, caption={Bash commands for turning off swapping in Linux Debian}]
% \end{lstlisting}

\section{Containerization of Services}
Container images are provided for running the application inside the \ac{CRI}. Definition of the container manifest is done in the "Containerfile"\footnote{Containerfile: https://www.mankier.com/5/Containerfile} format.

% TODO: ENTWEDER-ODER: Build has to be done inside the container. / All binary files are part of the container image.

\subsection{Code changes on core application}
For containerizing the \ac{OT} application, several changes were applied to its code base. This improves error handling and error tracing of the application and therefore simplifies development of the cluster. In the following sections, the several code changes are described in detail.

\subsubsection*{Introduction of exit codes}
The microservice \ac{DLL} files have return codes for different error cases. This is, for instance a code 200 in the \ac{AUTH} for connection issues to the database. As with process exit codes, a zero return code indicates a successful termination. Even though the main executable "open\_twin.exe" retrieves the return code, it did not convert these codes into proper process exit codes. 
Exit-Codes are a crucial part of the \ac{K8s}'s life cycle management as described in \autoref{chap:design.life_cycle}. Therefore, the return codes need to be converted into process exit codes.

The surrounding lines of code in the main executable are shown in \autoref{lst:code_changes.exitcodes.before}.
\begin{lstlisting}[label=lst:code_changes.exitcodes.before, caption={Former code snippet from main executable for calling the microservice library code in Rust (\textit{/Microservices/OpenTwin/src/main.rs})}, language=rust, firstnumber=85]
let _result = initialize(siteid_c_str.as_ptr(), service_c_str.as_ptr(), db_c_str.as_ptr(), dir_c_str.as_ptr());
\end{lstlisting}

A new condition for non-zero exit codes was added as shown in \autoref{lst:code_changes.exitcodes.after}. The variable \texttt{\_result}  contains the returned exit code of the library \ac{DLL}, whereas \texttt{lib\_path} contains the path to the library \ac{DLL}. As can be seen, it is used to describe the affected service name in a error message (line 87).
\begin{lstlisting}[label=lst:code_changes.exitcodes.after, caption={Code changes in Rust main executable for additional treatment of exit codes (\textit{/Microservices/OpenTwin/src/main.rs})}, language=rust, firstnumber=86]
if _result != 0 {
  eprintln!("Library/Service initialization ({}:init()) failed with exit code {}", lib_path, _result);
  std::process::exit(_result);
}
\end{lstlisting}

 
\subsubsection*{Debug verbosity in launcher}
By default Rust programs show a window for console output no matter if it was built in release mode or with debug parameters. However, the main executable uses conditional compilation to set configuration attributes about the windows subsystem.
Rust provides a compilation option "debug\_assertions" that is set to "true" for compilations without code optimization\cite{Rust.20230209}. Therefore it is set to "true" if the application runs in debug mode. As shown in \autoref{lst:cond_compilation_windows_subsystem} with conditional compilation, this build option is checked and console output is only shown for debug builds.
\begin{lstlisting}[label=lst:cond_compilation_windows_subsystem, caption={Conditional compilation for disabling console output in non-debug builds (\textit{/Microservices/OpenTwin/src/main.rs})}, language=rust, firstnumber=2]
#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]
\end{lstlisting}

Even if output is shown on the console window for debug builds only, the error messages are not able to be read conveniently in cases where the application crashes. This aggravates debugging of regular application errors outside of a containerized environment.
To avoid this behavior, the launcher batch files were adapted. For this, a new parameter was invented to the batch file for running the batch file in verbose mode.

\begin{lstlisting}[label=lst:launcher_extension.parameter_validation.after, caption={Additional command argument for preventing close of window after termination (\textit{/Microservices/Launcher/OpenTwin\_session.bat})}, language=cmd, firstnumber=18]
IF "%~1"=="/V" (
  REM OT is opening console windows in debug build, so we want to pause them at the end
  SET pause_prefix=cmd.exe /S /C "
  SET pause_suffix=" ^& pause
  ECHO ON
)
\end{lstlisting}
As first step, a new command line argument has been introduced. If "/V" is appended to the start of the launcher batch file it will run with higher verbosity.  As can be seen in \autoref{lst:launcher_extension.parameter_validation.after}, with "/V" appended, two new variables "pause\_prefix" and "pause\_suffix" are set (line 20-21). Furthermore, command output is enabled to debug the launcher file itself (line 22).

\begin{lstlisting}[label=lst:launcher_extension.call.after, caption={Additional command extension for preventing close of window after termination (\textit{/Microservices/Launcher/OpenTwin\_session.bat})}, language=cmd, firstnumber=34]
START "AUTHORIZATION SERVICE" %pause_prefix%open_twin.exe AuthorisationService.dll <*@ \Suppressnumber @*>
 "%OPEN_TWIN_SERVICES_ADDRESS%:%OPEN_TWIN_AUTH_PORT%" "%OPEN_TWIN_MONGODB_ADDRESS%"
 "%OPEN_TWIN_MONGODB_PWD%"%pause_suffix% <*@ \Reactivatenumber @*>
\end{lstlisting}
As can be seen in \autoref{lst:launcher_extension.call.after}, the newly defined variables "pause\_prefix" and "pause\_suffix" are appended to the command of starting a service. This ensures that a service process is started in a new window and the command is followed by the \ac{Windows} "pause" command to stop and wait for user interaction.


\subsubsection*{Enhanced logging and error tracing}
For improving the error tracing, the overall log amount has been increased. This involves enabling the logging of the central logging functions inside the library "OpenTwinCommunication". The logging mechanism did not use logging to standard output. Instead, calling the respective functions for logs were just ignored. This was changed and logging to standard output has been introduced.
Additionally, more calls to the logging mechanism, including caller information, have been added. For instance, the \ac{AUTH} now shows error messages for caught exceptions and errors during database initialization.
Also, the general error tracing has been improved. In the main services \ac{GSS}, \ac{LSS} and \ac{AUTH}, exception handling has been added, where it was not present before. Furthermore, the formatting of exception messages was improved and more information has been added.

For the current time being, there is an ongoing work to replace the logging to standard output by forwarding the log lines to a central logging library.

\subsubsection*{Certificate changes}
Since \ac{OT} is using \ac{mTLS} for communication between services, the \ac{CSR} file needs to contain proper host specifications. There is a script for the substitution of placeholders in a template \ac{CSR} file. However, this process does not work if performed for a containerized application. If the replacement takes place during the creation of the container, the final host name or \ac{IP} address does not yet exist. If, on the other hand, the automatic replacement is done later, it cannot be carried out from outside the container, as the script automatically replaces only the local host name.

%TODO: Wie dieses Problem gel√∂st?? Certificate template was extended by localhost/127.0.0.1???

\subsubsection*{Listening on all interfaces}
Container images have a certain network image for communicating to external hosts. Processes inside the container have to bind to this network interface. However, the \ac{IP} address of this network interface is unpredictable during compile time. The server processes that run inside a container therefore have to bind to all available network interfaces to be accessible to the outer network.
Furthermore, the services exchange service information with other services as described in \autoref{chap:background.baseline_architecture}. However, the binding address must differ from this published address in a containerized environment, since the binding address is mostly not accessible from the public. 

Instead of binding to all network interfaces, the main executable in \ac{OT} was only able to bind to a given \ac{IP} address (based on the published service address) only. Also, it was not possible to configure a different binding address than the one published to other services.

\autoref{lst:listen_all_interfaces.before} shows the affected lines of code in the main executable. The passed argument for the service \ac{URL} is forwarded to the server listener class. Afterwards, the address is printed on the console. The service \ac{URL}, here passed as variable, consists of the service address and the port of the service.
\begin{lstlisting}[label=lst:listen_all_interfaces.before, caption={Listener binding before the applied changes (\textit{Microservices/OpenTwin/src/main.rs})}, language=rust, firstnumber=145, numbers=left]
let listener = net::TcpListener::bind(&service_url).await?;
println!("Starting server at {:?}", service_url);
\end{lstlisting}

To not affect the outer interface, the changes work with the data already provided. This means, the passed arguments to the main executable do not require a change.
\begin{lstlisting}[label=lst:listen_all_interfaces.after, caption={Listener binding after the applied changes. The service url is parsed, based on its port. Binding is done on all interfaces. (\textit{Microservices/OpenTwin/src/main.rs})}, language=rust, firstnumber=150]
let service_port = Url::parse(&format!("https://{}", service_url))
.expect(&format!("Invalid service url. Unable to parse service url: {}", service_url))
.port();
if service_port.is_none() {
  panic!("Invalid service url. Service url is lacking port defintion: {}", service_url);
}
let binding_address = format!("0.0.0.0:{}", service_port.unwrap().to_string());
let listener = net::TcpListener::bind(&binding_address).await?;
println!("Server listening on {:?} (publishing {:?})", binding_address, service_url);
\end{lstlisting}
The binding address is separated from the published address, since the address in the argument still gets passed to the microservice \ac{DLL}. Afterwards, the service \ac{URL} is processed as shown in \autoref{lst:listen_all_interfaces.after}. First, the given service \ac{URL} is parsed and the port number is extracted. If no port is found or the parsing failed, the application fails and shows an error. Afterwards, the port number is concatenated with the binding address "0.0.0.0" and therefore the server binds to all addresses. The last line shows the new output containing the port number and the address published to other services.

% TODO - Bugfix for OpenGL error in uiFrontend (not necessary??)

\subsection{Container definition}
There are currently three container images prepared for containerization of \ac{OT}. These cover the main services for \ac{GSS}, \ac{LSS} and \ac{AUTH}. The structure is the same for each of the container files. The first part of the container file is shown in \autoref{lst:containerfile.1}.
\begin{lstlisting}[label=lst:containerfile.1, caption={Containerfile for the \ac{GSS}. Description of the base image and variable tagging using a build argument. (\textit{Distribution/Container/globalsession.Containerfile})}, language=docker, firstnumber=1]
ARG BASE_IMAGE_TAG=ltsc2022
FROM mcr.microsoft.com/windows/servercore:$BASE_IMAGE_TAG
\end{lstlisting}
The first line introduces a build argument for defining the target image tag from the command line without altering the container file.  It is used afterwards to pass the image tag to the base container image. The subsequent lines define labels for the resulting image.

\begin{comment}
\begin{lstlisting}[label=lst:containerfile.2, caption={Containerfile for the \ac{GSS}. First part describes the base image and the meta data of the image. (\textit{Distribution/Container/globalsession.Containerfile})}, language=docker, firstnumber=1]
LABEL maintainer="Frankfurt UAS"
LABEL app="open_twin.globalsession"

ENV SIM_PLAT_ROOT="" \
OPEN_TWIN_CERTS_PATH="C:\\app\\Certificates\\" \
OPEN_TWIN_CA_CERT="C:\\app\\Certificates\\ca.pem" \
OPEN_TWIN_SERVER_CERT="C:\\app\\Certificates\\server.pem" \
OPEN_TWIN_SERVER_CERT_KEY="C:\\app\\Certificates\\server-key.pem"

ENV OPEN_TWIN_GSS_SERVICE_ADDRESS=127.0.0.1 \
OPEN_TWIN_AUTH_SERVICE_ADDRESS=127.0.0.1 \
OPEN_TWIN_MONGODB_ADDRESS=tls@127.0.0.1:27017 \
OPEN_TWIN_GSS_PORT=8091 \
OPEN_TWIN_LSS_PORT=8093 \
OPEN_TWIN_AUTH_PORT=8092 \
OPEN_TWIN_GDS_PORT=9094 \
OPEN_TWIN_LDS_PORT=9095
\end{lstlisting}
\end{comment}

The container files differ in the runtime specification. They run the same entry point, but have a different process running as command. Furthermore, the exposed port depends on the service inside the container. 
After defining the command and copying the files into the container image, the certificates are built as part of the image file system. The discussed section of the container file can be seen in \autoref{lst:containerfile.3}.
\begin{lstlisting}[label=lst:containerfile.3, caption={Containerfile for the \ac{GSS}. Description of the command line and certificate creation. (\textit{Distribution/Container/globalsession.Containerfile})}, language=docker, firstnumber=21]
ENTRYPOINT ["cmd", "/C"]
CMD ["open_twin.exe", "GlobalSessionService.dll", "0", \
"%OPEN_TWIN_GSS_SERVICE_ADDRESS%:%OPEN_TWIN_GSS_PORT%", \
"%OPEN_TWIN_MONGODB_ADDRESS%", \
"%OPEN_TWIN_AUTH_SERVICE_ADDRESS%:%OPEN_TWIN_AUTH_PORT%"]
EXPOSE 8091
WORKDIR C:/app/Deployment/Certificates
COPY ./ ../
RUN createCertificate.bat && certutil -addstore root %OPEN_TWIN_CA_CERT%
WORKDIR C:/app/Deployment
\end{lstlisting}

% Container image must be built on each node machine manually. Since the Windows base image os is only compatible with the respective host operating system.
% Deployment-Verzeichnis muss daf√ºr initial auf Node kopiert werden.
% TODO: (Skript ("setup-node") daf√ºr schreiben, was neusten release von github zieht (RUN download?) und image autom. baut + image pusht?)  - base image (FROM) mittels args dynamisch gestalten?


\section{Cluster Setup}
The following section describes the setup of different machines in the cluster, so called nodes. While the master node refers to the \ac{K8s} Control-Plane node which is responsible for distribution of the workers, the worker nodes are the actual machines that are executing the applications. During development the cluster was set up on virtual machines completely, due to the lack of physical hardware.


\subsection{Creating the master node}
% At the current time being, the Kubernetes control plane must be based on linux system. Windows control planes are not supported.
For setting up the master node on Linux a system based on Debian Bullseye 11.5 has been used. After installing and setting up the operating system, the swap mechanism needs to be permanently turned off. This is done by editing the file system table (fstab) in file \tcode{/etc/fstab} respectively by commenting out the swap partitions and masking the systemd swap units.

% sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
% sudo systemctl mask dev-sda3.swap

% https://www.natarajmb.com/2022/06/kubernetes-debian/
%$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf 
%overlay 
%br_netfilter 
%EOF 

%$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf 
%net.ipv4.ip_forward                 = 1 
%net.netfilter.nf_conntrack_max      = 524288 
%EOF
%


% TODO: Define the prerequisite packages: containerd
After installing the prerequisite packages, a ContainerD configuration file needs to be created. For this, the command from \autoref{lst:master.containerd_config} is applied.
\begin{lstlisting}[label=lst:master.containerd_config, caption={Bash command for setting up containerd config}]
sudo sysctl net.bridge.bridge-nf-call-iptables=1
echo 1 > /proc/sys/net/ipv4/ip_forward
sudo containerd config default | sudo tee /etc/containerd/config.toml &>/dev/null
\end{lstlisting}
Afterwards the systemd \ac{cgroup} is added to the runtime options of ContainerD and the its service is restarted.
%\begin{lstlisting}[label=lst:containerd_config, caption={Bash command for setting up containerd config}]
%sudo containerd config default | 
%sed 's/\[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options\]/&\n            SystemdCgroup = true/' |
% sudo tee /etc/containerd/config.toml >/dev/null
% sudo service containerd restart
%\end{lstlisting}
After setting up the prerequisites, the cluster can be initialized by running the command line tool as shown in \autoref{lst:master.kubeadm.init} with the appropriate configuration as parameter.
\begin{lstlisting}[label=lst:master.kubeadm.init, caption={Bash command for setting up the cluster}]
sudo kubeadm init --config config.yaml
\end{lstlisting}

\subsubsection{Installing a Container Network Interface}
After successfully running the initialization, the cluster overlay network \tcode{flannel} needs to be setup.
This is required for working with \ac{Windows} worker nodes.  To setup \tcode{flannel} the respective pod description can be directly downloaded from the vendor\footnote{https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml}. In the configuration the \ac{VNI} (4096) and port (4789) for Flannel on \ac{Windows} were set. Afterwards the configuration has been applied on the cluster.
After linking kubectl to the local control plane node, the successful setup of the cluster can be checked with the \tcode{kubectl} command.


\subsubsection{Adding the proxy daemonsets}
For using \ac{Windows} worker nodes in a \ac{K8s} cluster, additional configuration mappings ("configmaps") and daemon sets need to be applied on the cluster. Those are used for setting up a proxy for \tcode{flannel}. The additional objects are applied running the two commands shown in \autoref{lst:master.proxy_daemonsets}
\begin{lstlisting}[label=lst:master.proxy_daemonsets, caption={Bash command for adding the flannel overlay configuration\cite{GitHubKubernetesSIGWindowsTools.20230213}}]
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest \
  /download/kube-proxy.yml | sed 's/VERSION/v1.25.3/g' | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases \
  /latest/download/flannel-overlay.yml
\end{lstlisting}

\subsection{Creating the worker node}
% Prereqs: Hyper-V,  Containers, crictl
On the Windows worker node, the prerequisites were installed first. While installing the prerequisite \tcode{crictl} it is added to the \tcode{PATH} environment variable. After the setup, the node preparation scripts of the \ac{K8s} \ac{SIG} are retrieved. Before running the scripts, the \ac{CNI} version needs to be aligned to the same version running on the master node. Therefore the appearances of "v0.2.0" is replaced with "v0.3.0" respectively.
Afterwards, the installation script is executed. It sets up the \ac{NAT} configuration on the worker node and registers ContainerD as a service.
% [plugins."io.containerd.grpc.v1.cri"].sandbox_image
For having a valid image at a later point in time, the \ac{CRI} value "sandbox\_image" in ContainerD's configuration file (\textit{config.toml}) needs to be replaced with a newer version.

After successfully setting up the \ac{NAT} and installing ContainerD as a service, the node will be finally prepared to host tasks. For this, another Powershell script "PrepareNode"\footnote{https://github.com/kubernetes-sigs/sig-windows-tools/releases/download/v0.1.5/PrepareNode.ps1} from the Kubernetes \ac{SIG} runs. After running the script, the resulting "StartKubelet" file needs to be changed to drop invalid arguments.

% C:\k\kubelet.exe $global:KubeletArgs --cert-dir=$env:SYSTEMDRIVE\var\lib\kubelet\pki --config=/var/lib/kubelet/config.yaml --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --hostname-override=$(hostname)
Furthermore, the following lines were added to the Kubelet configuration:
\begin{lstlisting}[label=lst:master.configtoml_changes, caption={Configuration changes in ContainerD configuration file (\textit{config.toml})\cite{GitHubKubernetesSIGWindowsTools.20230213}}]
enforceNodeAllocatable: []
cgroupsPerQOS: false
enableDebuggingHandlers: true
\end{lstlisting}
These configuration values are valid for \ac{Windows} machines only and cause an error in Kubelet on Linux. The configuration changes therefore can only be served to Windows machines and the configuration on the nodes needs to be changed locally.

% Es wurde "New-HnsNetwork -Type NAT -Name nat -Gateway 10.244.0.1 -AdressPrefix 10.244.240.0/20" ausgef√ºhrt
% TODO: Evtl. testen, ob man mit Docker auf Netzwerk zugreifen kann? "Object already exists"
% TODO: Evtl.- auf calico wechseln? Oder worker node als hostprocess ausf√ºhren
% TODO: -- siehe: https://github.com/kubernetes-sigs/sig-windows-tools/issues/128

After successful run of the preparation script the node is ready to join the cluster.


\section{Automatic setup}
% TODO: Powershell script beschreiben



