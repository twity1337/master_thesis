\chapter{Implementation} % Main chapter title

\label{chap:implementation} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref*{chap:implementation}. \emph{Implementation}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

% \begin{lstlisting}[label={lst:swapoff}, caption={Bash commands for turning off swapping in Linux Debian}]
% \end{lstlisting}

\section{Containerization of Services}
Container images are provided for running the application inside the \ac{CRI}. Definition of the container manifest is done in the "Containerfile"\footnote{Containerfile: https://www.mankier.com/5/Containerfile} format.

% TODO: ENTWEDER-ODER: Build has to be done inside the container. / All binary files are part of the container image.

\subsection{Code changes on core application}
For containerizing the \ac{OT} application, several changes were applied to its code base. This improves error handling and error tracing of the application and therefore simplifies development of the cluster. In the following sections, the several code changes are described in detail.

\subsubsection*{Introduction of exit codes}
The microservice \ac{DLL} files have return codes for different error cases. This is, for instance a code 200 in the \ac{AUTH} for connection issues to the database. As with process exit codes, a zero return code indicates a successful termination. Even though the main executable "open\_twin.exe" retrieves the return code, it did not convert these codes into proper process exit codes. 
Exit-Codes are a crucial part of the \ac{K8s}'s life cycle management as described in \autoref{chap:design.life_cycle}.

The surrounding lines of code in the main executable are shown in \autoref{lst:code_changes.exitcodes.before}.
\begin{lstlisting}[label=lst:code_changes.exitcodes.before, caption={Former code snippet from main executable for calling the microservice library code in Rust (/Microservices/OpenTwin/src/main.rs)}, language=rust, firstnumber=85]
let _result = initialize(siteid_c_str.as_ptr(), service_c_str.as_ptr(), db_c_str.as_ptr(), dir_c_str.as_ptr());
\end{lstlisting}

A new condition for non-zero exit codes was added as shown in \autoref{lst:code_changes.exitcodes.after}. The variable \texttt{\_result}  contains the returned exit code of the library \ac{DLL}, whereas \texttt{lib\_path} contains the path to the library \ac{DLL}. As can be seen, it is used to describe the affected service name in a error message (line 87).
\begin{lstlisting}[label=lst:code_changes.exitcodes.after, caption={Code changes in Rust main executable for additional treatment of exit codes (/Microservices/OpenTwin/src/main.rs)}, language=rust, firstnumber=86]
if _result != 0 {
  eprintln!("Library/Service initialization ({}:init()) failed with exit code {}", lib_path, _result);
  std::process::exit(_result);
}
\end{lstlisting}

 
\subsubsection*{Debug verbosity in launcher}
By default Rust programs show a window for console output no matter if it was built in release mode or with debug parameters. However, the main executable uses conditional compilation to set configuration attributes about the windows subsystem.
Rust provides a compilation option "debug\_assertions" that is set to "true" for compilations without code optimization\cite{Rust.20230209}. Therefore it is set to "true" if the application runs in debug mode. As shown in \autoref{lst:cond_compilation_windows_subsystem} with conditional compilation, this build option is checked and console output is only shown for debug builds.
\begin{lstlisting}[label=lst:cond_compilation_windows_subsystem, caption={Conditional compilation for disabling console output in non-debug builds (/Microservices/OpenTwin/src/main.rs)}, language=rust, firstnumber=2]
#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]
\end{lstlisting}

Even if output is shown on the console window for debug builds only, the error messages are not able to be read conveniently in cases where the application crashes. This aggravates debugging of regular application errors outside of a containerized environment.
To avoid this behavior, the launcher batch files were adapted. For this, a new parameter was invented to the batch file for running the batch file in verbose mode.

\begin{lstlisting}[label=lst:launcher_extension.parameter_validation.after, caption={Additional command argument for preventing close of window after termination (/Microservices/Launcher/OpenTwin\_session.bat)}, language=cmd, firstnumber=18]
IF "%~1"=="/V" (
  REM OT is opening console windows in debug build, so we want to pause them at the end
  SET pause_prefix=cmd.exe /S /C "
  SET pause_suffix=" ^& pause
  ECHO ON
)
\end{lstlisting}
As first step, a new command line argument has been introduced. If "/V" is appended to the start of the launcher batch file it will run with higher verbosity.  As can be seen in \autoref{lst:launcher_extension.parameter_validation.after}, with "/V" appended, two new variables "pause\_prefix" and "pause\_suffix" are set (line 20-21). Furthermore, command output is enabled to debug the launcher file itself (line 22).

\begin{lstlisting}[label=lst:launcher_extension.call.after, caption={Additional command extension for preventing close of window after termination (/Microservices/Launcher/OpenTwin\_session.bat)}, language=cmd, firstnumber=34]
START "AUTHORIZATION SERVICE" %pause_prefix%open_twin.exe AuthorisationService.dll <*@ \Suppressnumber @*>
 "%OPEN_TWIN_SERVICES_ADDRESS%:%OPEN_TWIN_AUTH_PORT%" "%OPEN_TWIN_MONGODB_ADDRESS%"
 "%OPEN_TWIN_MONGODB_PWD%"%pause_suffix%
\end{lstlisting}
As can be seen in \autoref{lst:launcher_extension.call.after}, the newly defined variables "pause\_prefix" and "pause\_suffix" are appended to the command of starting a service. This ensures that a service process is started in a new window and the command is followed by the \ac{Windows} "pause" command to stop and wait for user interaction.


\subsubsection*{Enhanced logging and error tracing}
For improving the error tracing, the overall log amount has been increased. This involves enabling the logging of the central logging functions inside the library "OpenTwinCommunication". The logging mechanism did not use logging to standard output. Instead, calling the respective functions for logs were just ignored. This was changed and logging to standard output has been introduced.
Additionally, more calls to the logging mechanism, including caller information, have been added. For instance, the \ac{AUTH} now shows error messages for caught exceptions and errors during database initialization.
Also, the general error tracing has been improved. In the main services \ac{GSS}, \ac{LSS} and \ac{AUTH}, exception handling has been added, where it was not present before. Furthermore, the formatting of exception messages was improved and more information has been added.

For the current time being, there is an ongoing work to replace the logging to standard output by forwarding the log lines to a central logging library.

\subsubsection*{Certificate changes}
- Certificate template was extended by localhost/127.0.0.1


- OpenTwin excutable (rust server) listens on all interface (because of Docker); Port parsing
- Bugfix for OpenGL error in uiFrontend

\subsection{Container definition}


% Container image must be built on each node machine manually. Since the Windows base image os is only compatible with the respective host operating system.
% Deployment-Verzeichnis muss dafür initial auf Node kopiert werden.
% TODO: (Skript ("setup-node") dafür schreiben, was neusten release von github zieht (RUN download?) und image autom. baut + image pusht?)  - base image (FROM) mittels args dynamisch gestalten?
% dabei beachten, dass Klammern gesetzt werden (Json-Style) bei Entrypoint, da sonst zwar der Build bei docker funktioniert, aber das Image nicht in containerd läuft.
% nach Bau von Image muss dieses in richtigen containerd namespace verschoben werden, wenn image lokal vorhanden und nicht von registry gezogen wird.


% TODO: Skript zum bauen des Images beschreiben
% TODO: Containerfile





\section{Cluster Setup}
The following section describes the setup of different machines in the cluster, so called nodes. While the master node refers to the \ac{K8s} Control-Plane node which is responsible for distribution of the workers, the worker nodes are the actual machines that are executing the applications. During development the cluster was set up on virtual machines completely, due to the lack of physical hardware.


\subsection{Creating the master node}
% At the current time being, the Kubernetes control plane must be based on linux system. Windows control planes are not supported.
For setting up the master node on Linux a system based on Debian Bullseye 11.5 has been used. After installing and setting up the operating system, the swap mechanism needs to be permanently turned off. This is done by editing the file system table (fstab) in file \tcode{/etc/fstab} respectively by commenting out the swap partitions and masking the systemd swap units.

% sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
% sudo systemctl mask dev-sda3.swap

% https://www.natarajmb.com/2022/06/kubernetes-debian/
%$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf 
%overlay 
%br_netfilter 
%EOF 

%$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf 
%net.ipv4.ip_forward                 = 1 
%net.netfilter.nf_conntrack_max      = 524288 
%EOF
%


% TODO: Define the prerequisite packages: containerd
After installing the pre-requisite packages, a containerd config file needs to be created. For this, the command from \autoref{lst:master.containerd_config} is applied.
\begin{lstlisting}[label=lst:master.containerd_config, caption={Bash command for setting up containerd config}]
sudo sysctl net.bridge.bridge-nf-call-iptables=1
echo 1 > /proc/sys/net/ipv4/ip_forward
sudo containerd config default | sudo tee /etc/containerd/config.toml &>/dev/null
\end{lstlisting}
Afterwards the systemd \ac{cgroup} is added to the runtime options of containerd and the its service is restarted.
%\begin{lstlisting}[label=lst:containerd_config, caption={Bash command for setting up containerd config}]
%sudo containerd config default | 
%sed 's/\[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options\]/&\n            SystemdCgroup = true/' |
% sudo tee /etc/containerd/config.toml >/dev/null
% sudo service containerd restart
%\end{lstlisting}
After setting up the prerequisites, the cluster can be initialized by running the command line tool as shown in \autoref{lst:master.kubeadm.init} with the appropriate configuration as parameter.
\begin{lstlisting}[label=lst:master.kubeadm.init, caption={Bash command for setting up the cluster}]
sudo kubeadm init --config config.yaml
\end{lstlisting}

\subsubsection{Installing a Container Network Interface}
After successfully running the initialization, the cluster overlay network \tcode{flannel} needs to be setup.
This is required for working with Windows worker nodes.  To setup \tcode{flannel} the respective pod description can be directly downloaded from the vendor\footnote{https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml}. In the configuration the VNI (4096) and port (4789) for Flannel on Windows were set. Afterwards the configuration has been applied on the cluster.
After linking kubectl to the local control plane node, the successful setup of the cluster can be checked with the \tcode{kubectl} command.


\subsubsection{Adding the proxy daemonsets}
For using Windows worker nodes in a Kubernetes cluster, additional configmaps and daemonsets need to be applied on the cluster. Those are used for setting up a proxy for \tcode{flannel}.
\begin{lstlisting}
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest \
  /download/kube-proxy.yml | sed 's/VERSION/v1.25.3/g' | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases \
  /latest/download/flannel-overlay.yml
\end{lstlisting}

\subsection{Creating the worker node}
% Prereqs: Hyper-V,  Containers, crictl
On the Windows worker node, the prerequisites were installed first. While installing the prerequisite \tcode{crictl} it was added to the \tcode{PATH} environment variable. After the setup, the node preparation scripts of the Kubernetes \ac{SIG} were retrieved. Before running the scripts, the \ac{CNI} version needs to be aligned to the same version written on the master node. Therefore the appearances of v0.2.0 have been replaced with v0.3.0 respectively.
Afterwards the installation script was executed. It sets up the \ac{NAT} configuration on the worker node and registers containerd as a service.
% [plugins."io.containerd.grpc.v1.cri"].sandbox_image
For having a valid image at a later point in time, the value sandbox\_image in cri in containerd's configuration file (config.toml) needs to be replaced with a newer version.

After sucessfully setting up the \ac{NAT} and installing containerd as a service, the node will be finally prepared to host tasks. For this, another powershell script "PrepareNode"\footnote{https://github.com/kubernetes-sigs/sig-windows-tools/releases/download/v0.1.5/PrepareNode.ps1} from the Kubernetes \ac{SIG} was run. After running the script the resulting "StartKubelet" file needs to be changed to drop invalid arguments.

% C:\k\kubelet.exe $global:KubeletArgs --cert-dir=$env:SYSTEMDRIVE\var\lib\kubelet\pki --config=/var/lib/kubelet/config.yaml --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --hostname-override=$(hostname)
Furthermore, the following lines were added to the Kubelet configuration:
\begin{lstlisting}
enforceNodeAllocatable: []
cgroupsPerQOS: false
enableDebuggingHandlers: true
\end{lstlisting}
Since the configuration changes needs to be served only to Windows machines (config value are valid for Windows only) we (!!WE!!) need to manually change the configuration on the nodes locally. This

% Es wurde "New-HnsNetwork -Type NAT -Name nat -Gateway 10.244.0.1 -AdressPrefix 10.244.240.0/20" ausgeführt
% TODO: Evtl. testen, ob man mit Docker auf Netzwerk zugreifen kann? "Object already exists"
% TODO: Evtl.- auf calico wechseln? Oder worker node als hostprocess ausführen
% TODO: -- siehe: https://github.com/kubernetes-sigs/sig-windows-tools/issues/128

After successful run of the preparation script the node was ready to join the cluster.


\section{Automatic setup}
% TODO: Powershell script beschreiben



