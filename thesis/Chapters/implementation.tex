\chapter{Implementation} % Main chapter title

\label{chap:implementation} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref*{chap:implementation}. \emph{Implementation}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

% \begin{lstlisting}[label={lst:swapoff}, caption={Bash commands for turning off swapping in Linux Debian}]
% \end{lstlisting}

\section{Containerization of Services}
Container images are provided for running the application inside the \ac{CRI}. Definition of the container manifest is done in the "Containerfile"\footnote{https://www.mankier.com/5/Containerfile} format is used.
- Build has to be done inside the container. / All binary files are part of the container image.

\section{Cluster Setup}
The following section describes the setup of different machines in the cluster, so called nodes. While the master node refers to the \ac{K8s} Control-Plane node which is responsible for distribution of the workers, the worker nodes are the actual machines that are executing the applications. During development the cluster was set up on virtual machines completely, due to the lack of physical hardware.


\subsection{Creating the master node}
% At the current time being, the Kubernetes control plane must be based on linux system. Windows control planes are not supported.
For setting up the master node on Linux a system based on Debian Bullseye 11.5 has been used. After installing and setting up the operating system, the swap mechanism needs to be permanently turned off. This is done by editing the file system table (fstab) in file \tcode{/etc/fstab} respectively by commenting out the swap partitions and masking the systemd swap units.

% sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
% sudo systemctl mask dev-sda3.swap

% https://www.natarajmb.com/2022/06/kubernetes-debian/
%$ cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf 
%overlay 
%br_netfilter 
%EOF 

%$ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf 
%net.ipv4.ip_forward                 = 1 
%net.netfilter.nf_conntrack_max      = 524288 
%EOF
%


% TODO: Define the prerequisite packages: containerd
After installing the pre-requisite packages, a containerd config file needs to be created. For this, the command from \autoref{lst:master.containerd_config} is applied.
\begin{lstlisting}[label=lst:master.containerd_config, caption={Bash command for setting up containerd config}]
sudo sysctl net.bridge.bridge-nf-call-iptables=1
echo 1 > /proc/sys/net/ipv4/ip_forward
sudo containerd config default | sudo tee /etc/containerd/config.toml &>/dev/null
\end{lstlisting}
Afterwards the systemd \ac{cgroup} is added to the runtime options of containerd and the its service is restarted.
%\begin{lstlisting}[label=lst:containerd_config, caption={Bash command for setting up containerd config}]
%sudo containerd config default | 
%sed 's/\[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options\]/&\n            SystemdCgroup = true/' |
% sudo tee /etc/containerd/config.toml >/dev/null
% sudo service containerd restart
%\end{lstlisting}
After setting up the prerequisites, the cluster can be initialized by running the command line tool as shown in \autoref{lst:master.kubeadm.init} with the appropriate configuration as parameter.
\begin{lstlisting}[label=lst:master.kubeadm.init, caption={Bash command for setting up the cluster}]
sudo kubeadm init --config config.yaml
\end{lstlisting}

\subsubsection{Installing a Container Network Interface}
After successfully running the initialization, the cluster overlay network \tcode{flannel} needs to be setup.
This is required for working with Windows worker nodes.  To setup \tcode{flannel} the respective pod description can be directly downloaded from the vendor\footnote{https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml}. In the configuration the VNI (4096) and port (4789) for Flannel on Windows were set. Afterwards the configuration has been applied on the cluster.
After linking kubectl to the local control plane node, the successful setup of the cluster can be checked with the \tcode{kubectl} command.


\subsubsection{Adding the proxy daemonsets}
For using Windows worker nodes in a Kubernetes cluster, additional configmaps and daemonsets need to be applied on the cluster. Those are used for setting up a proxy for \tcode{flannel}.
\begin{lstlisting}
curl -L https://github.com/kubernetes-sigs/sig-windows-tools/releases/latest \
  /download/kube-proxy.yml | sed 's/VERSION/v1.25.3/g' | kubectl apply -f -
kubectl apply -f https://github.com/kubernetes-sigs/sig-windows-tools/releases \
  /latest/download/flannel-overlay.yml
\end{lstlisting}

\subsection{Creating the worker node}
% Prereqs: Hyper-V,  Containers, crictl
On the Windows worker node, the prerequisites were installed first. While installing the prerequisite \tcode{crictl} it was added to the \tcode{PATH} environment variable. After the setup, the node preparation scripts of the Kubernetes \ac{SIG} were retrieved. Before running the scripts, the \ac{CNI} version needs to be aligned to the same version written on the master node. Therefore the appearances of v0.2.0 have been replaced with v0.3.0 respectively.
Afterwards the installation script was executed. It sets up the \ac{NAT} configuration on the worker node and registers containerd as a service.
% [plugins."io.containerd.grpc.v1.cri"].sandbox_image
For having a valid image at a later point in time, the value sandbox\_image in cri in containerd's configuration file (config.toml) needs to be replaced with a newer version.

After sucessfully setting up the \ac{NAT} and installing containerd as a service, the node will be finally prepared to host tasks. For this, another powershell script "PrepareNode"\footnote{https://github.com/kubernetes-sigs/sig-windows-tools/releases/download/v0.1.5/PrepareNode.ps1} from the Kubernetes \ac{SIG} was run. After running the script the resulting "StartKubelet" file needs to be changed to drop invalid arguments.

% C:\k\kubelet.exe $global:KubeletArgs --cert-dir=$env:SYSTEMDRIVE\var\lib\kubelet\pki --config=/var/lib/kubelet/config.yaml --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --hostname-override=$(hostname)
Furthermore, the following lines were added to the Kubelet configuration:
\begin{lstlisting}
enforceNodeAllocatable: []
cgroupsPerQOS: false
enableDebuggingHandlers: true
\end{lstlisting}
Since the configuration changes needs to be served only to Windows machines (config value are valid for Windows only) we (!!WE!!) need to manually change the configuration on the nodes locally. This

% Es wurde "New-HnsNetwork -Type NAT -Name nat -Gateway 10.244.0.1 -AdressPrefix 10.244.240.0/20" ausgeführt
% TODO: Evtl. testen, ob man mit Docker auf Netzwerk zugreifen kann? "Object already exists"
% TODO: Evtl.- auf calico wechseln? Oder worker node als hostprocess ausführen
% TODO: -- siehe: https://github.com/kubernetes-sigs/sig-windows-tools/issues/128

After successful run of the preparation script the node was ready to join the cluster.

\section{Containerization of Application}
% For containerizing the OpenTwin application, several changes were applied to the code base.



% Container image must be built on each node machine manually. Since the Windows base image os is only compatible with the respective host os.
% Deployment-Verzeichnis muss dafür initial auf Node kopiert werden.
% TODO: (Skript ("setup-node") dafür schreiben, was neusten release von github zieht (RUN download?) und image autom. baut + image pusht?)  - base image (FROM) mittels args dynamisch gestalten?
% dabei beachten, dass Klammern gesetzt werden (Json-Style) bei Entrypoint, da sonst zwar der Build bei docker funktioniert, aber das Image nicht in containerd läuft.
% nach Bau von Image muss dieses in richtigen containerd namespace verschoben werden, wenn image lokal vorhanden und nicht von registry gezogen wird.


